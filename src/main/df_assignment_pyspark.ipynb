{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f320c6f0-efec-4096-a471-c33d582e24e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()  # Auto-detects SPARK_HOME\n",
    "\n",
    "from typing import List\n",
    "from pprint import pprint\n",
    "import json\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca23981e-1b64-4593-8df1-7c2fb9c8e10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _id: struct (nullable = true)\n",
      " |    |-- $oid: string (nullable = true)\n",
      " |-- author: struct (nullable = true)\n",
      " |    |-- avatar_url: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- login: string (nullable = true)\n",
      " |    |-- site_admin: boolean (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- commit: struct (nullable = true)\n",
      " |    |-- author: struct (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |-- comment_count: long (nullable = true)\n",
      " |    |-- committer: struct (nullable = true)\n",
      " |    |    |-- date: string (nullable = true)\n",
      " |    |    |-- email: string (nullable = true)\n",
      " |    |    |-- name: string (nullable = true)\n",
      " |    |-- message: string (nullable = true)\n",
      " |    |-- tree: struct (nullable = true)\n",
      " |    |    |-- sha: string (nullable = true)\n",
      " |    |-- verification: struct (nullable = true)\n",
      " |    |    |-- payload: string (nullable = true)\n",
      " |    |    |-- reason: string (nullable = true)\n",
      " |    |    |-- signature: string (nullable = true)\n",
      " |    |    |-- verified: boolean (nullable = true)\n",
      " |-- committer: struct (nullable = true)\n",
      " |    |-- avatar_url: string (nullable = true)\n",
      " |    |-- id: long (nullable = true)\n",
      " |    |-- login: string (nullable = true)\n",
      " |    |-- site_admin: boolean (nullable = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- files: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- additions: long (nullable = true)\n",
      " |    |    |-- blob_url: string (nullable = true)\n",
      " |    |    |-- changes: long (nullable = true)\n",
      " |    |    |-- contents_url: string (nullable = true)\n",
      " |    |    |-- deletions: long (nullable = true)\n",
      " |    |    |-- filename: string (nullable = true)\n",
      " |    |    |-- patch: string (nullable = true)\n",
      " |    |    |-- raw_url: string (nullable = true)\n",
      " |    |    |-- sha: string (nullable = true)\n",
      " |    |    |-- status: string (nullable = true)\n",
      " |-- node_id: string (nullable = true)\n",
      " |-- parents: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- sha: string (nullable = true)\n",
      " |-- sha: string (nullable = true)\n",
      " |-- stats: struct (nullable = true)\n",
      " |    |-- additions: long (nullable = true)\n",
      " |    |-- deletions: long (nullable = true)\n",
      " |    |-- total: long (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"PySpark DF Assignment\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# first load to DF because we have a multiline json\n",
    "commits_df = spark.read.option(\"multiline\", \"true\") \\\n",
    "      .json(\"../data/data_raw.json\")\n",
    "\n",
    "commits_df.createOrReplaceTempView(\"commits\")\n",
    "\n",
    "# show df schema\n",
    "commits_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0652cb23-3eb6-44a9-96ed-939364f1bbed",
   "metadata": {},
   "source": [
    "### Assignment 1\n",
    "In this exercise we want to know all the commit SHA's from a list of commit committers. We require these to be in order according to timestamp.\n",
    "\n",
    "| committer      | sha                                      | timestamp            |\n",
    "|----------------|------------------------------------------|----------------------|\n",
    "| Harbar-Inbound | 1d8e15a834a2157fe7af04421c42a893e8a1f23a | 2019-03-10T15:24:16Z |\n",
    "| ...            | ...                                      | ...                  |\n",
    "\n",
    "Hint: try to work out the individual stages of the exercises, which makes it easier to track bugs, and figure out how Spark Dataframes and their operations work. You can also use the `printSchema()` function and `show()`function to take a look at the structure and contents of the Dataframes.\n",
    "\n",
    "@param commits Commit Dataframe, created from the data_raw.json file.\n",
    "\n",
    "@param authors Sequence of String representing the authors from which we want to know their respective commit SHA's.\n",
    "\n",
    "@return DataFrame of commits from the requested authors, including the commit SHA and the according timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "45dea7da-40a5-43e9-a745-5a2e7c3f6b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_1(commits_df, authors: List[str]):\n",
    "\n",
    "    # filter based on authors\n",
    "    filtered_df = commits_df.filter(commits_df[\"commit.committer.name\"].isin(*authors))\n",
    "    filtered_df.createOrReplaceTempView(\"filtered_commits\")\n",
    "\n",
    "    # select relevant data: committer (commit/committer/name), sha, commit/committer/date\n",
    "    # order by timestamp\n",
    "    query = \"\"\"\n",
    "    SELECT  \n",
    "        commit.committer.name as name,\n",
    "        sha,\n",
    "        commit.committer.date as time                                       \n",
    "    FROM filtered_commits\n",
    "    ORDER BY time\n",
    "    \"\"\"\n",
    "    \n",
    "    result_df = spark.sql(query)\n",
    "        \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9d78a58-4679-4f9c-83b7-36b6aac40821",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_top2 = [(\"GitHub\", \"65a88dce81af6d4dc6751deab202fa8eeadaf9c0\", \"2019-01-27T07:09:13.000Z\"),\n",
    "                 (\"GitHub\", \"54fc3ef7d48b1c25910485b8f369023077d0f995\", \"2019-03-04T15:21:52.000Z\")]\n",
    "result_df = assignment_1(commits_df, [\"GitHub\"]).collect()\n",
    "assert tuple(result_df[0]) == expected_top2[0]\n",
    "assert tuple(result_df[1]) == expected_top2[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655e786-b504-4553-a208-573416e24377",
   "metadata": {},
   "source": [
    "### Assignment 2\n",
    "\n",
    "In order to generate weekly dashboards for all projects, we need the data to be partitioned by weeks. As projects can span multiple years in the data set, care must be taken to partition by not only weeks but also by years.\n",
    "\n",
    "The returned DataFrame that is expected is in the following format:\n",
    "\n",
    "| repository | week             | year | count   |\n",
    "|------------|------------------|------|---------|\n",
    "| Maven      | 41               | 2019 | 21      |\n",
    "| .....      | ..               | .... | ..      |\n",
    "\n",
    "@param commits Commit Dataframe, created from the data_raw.json file.\n",
    "\n",
    "@return Dataframe containing 4 columns, Repository name, week number, year and the number fo commits for that week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "5245e56d-8e43-414b-9f92-3b57e6065ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_2():\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            SPLIT_PART(url, '/', 6) as repo,\n",
    "            weekofyear(commit.committer.date) as week,\n",
    "            YEAR(commit.committer.date) as year,\n",
    "            COUNT(*) as count\n",
    "        FROM commits\n",
    "        GROUP BY repo, week, year\n",
    "    \n",
    "    \"\"\"\n",
    "    result_df = spark.sql(query)\n",
    "    return result_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b5125e42-4acc-4b92-a35d-51ec2ef35a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# test\n",
    "result = assignment_2()\n",
    "result_set = set(result.collect())\n",
    "expected_set = {(\"DrTests\", 21, 2019, 1), \n",
    "                (\"ExoGit\", 21, 2019, 6)}\n",
    "\n",
    "assert expected_set.issubset(result_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435c28a-3bad-492f-b10e-77129da70ff7",
   "metadata": {},
   "source": [
    "### Assignment 3\n",
    "\n",
    "A developer is interested in the age of commits in seconds. Although this can always be calculated during runtime, it would require passing a `Timestamp` (in milliseconds) along with the computation. Therefore, we require you to append the inputted DataFrame with an `age` column representing the age of each commit in seconds.\n",
    "\n",
    "##### Expected DataFrame Example:\n",
    "\n",
    "| age  |\n",
    "|------|\n",
    "| 1231 |\n",
    "| 20   |\n",
    "| ...  |\n",
    "\n",
    "#### Parameters:\n",
    "- **commits**: Commit DataFrame, created from the `data_raw.json` file.\n",
    "\n",
    "#### Returns:\n",
    "- The inputted DataFrame appended with an `age` column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "3e6be727-ca01-4b64-8b10-65cb8182e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_3(timestamp_ms):\n",
    "    query = f\"\"\"\n",
    "        SELECT \n",
    "            *,\n",
    "            DATEDIFF(SECOND, commit.committer.date, FROM_UNIXTIME({timestamp_ms}/ 1000)) as age\n",
    "        FROM commits\n",
    "    \"\"\"\n",
    "    result_df = spark.sql(query)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "71e59229-93f9-411f-9d05-71aea859577b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# test\n",
    "expected = [12093031, 12093019, 12093032, 12093030]\n",
    "\n",
    "result = assignment_3(timestamp_ms=1570707029000)\n",
    "first_4_results = result.take(4)\n",
    "\n",
    "for res, exp in zip(first_4_results, expected):\n",
    "    assert res[\"age\"] == exp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c0eb5d-32fc-4600-9aa3-e06155569477",
   "metadata": {},
   "source": [
    "### Assignment 4\n",
    "\n",
    "To perform analysis on commit behavior, the intermediate time between commits is needed. We require that the DataFrame input be appended with an extra column that expresses the number of days between the current commit and the previous commit of the user, independent of the branch or repository. If no commit exists before a commit, regard the time difference in days as zero. Make sure to return the commits in chronological order.\n",
    "\n",
    "**Hint:** Look into Spark SQL's `Window` to have more expressive power in custom aggregations.\n",
    "\n",
    "### Expected DataFrame Example:\n",
    "\n",
    "| $oid                      | name   | date                       | time_diff |\n",
    "|---------------------------|--------|----------------------------|-----------|\n",
    "| 5ce6929e6480fd0d91d3106a   | GitHub | 2019-01-27T07:09:13.000Z   | 0         |\n",
    "| 5ce693156480fd0d5edbd708   | GitHub | 2019-03-04T15:21:52.000Z   | 36        |\n",
    "| 5ce691b06480fd0fe0972350   | GitHub | 2019-03-06T13:55:25.000Z   | 2         |\n",
    "| ...                       | ...    | ...                        | ...       |\n",
    "\n",
    "#### Parameters:\n",
    "- **commits**: Commit DataFrame (see `commit.json` and `data_raw.json` for the structure of the file, or run `println(commits.schema)`).\n",
    "- **authorName**: Name of the author for which the result must be generated.\n",
    "\n",
    "#### Returns:\n",
    "- DataFrame with a column expressing the days since the last commit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "af9bebf2-1259-4bee-a96e-fd55298494a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_4(author_name):\n",
    "    query = f\"\"\"\n",
    "        WITH commits_with_lag (\n",
    "            SELECT \n",
    "                _id as id,\n",
    "                commit.committer.name as name,\n",
    "                CAST(commit.committer.date AS date) as date,\n",
    "                LAG(commit.committer.date) OVER \n",
    "                    (PARTITION BY commit.committer.name\n",
    "                    ORDER BY commit.committer.date) as prev_commit_date\n",
    "            FROM commits\n",
    "            WHERE commit.committer.name = '{author_name}'\n",
    "        )\n",
    "\n",
    "        SELECT \n",
    "            id,\n",
    "            name,\n",
    "            date,\n",
    "            CASE\n",
    "                WHEN prev_commit_date IS NULL THEN 0\n",
    "                WHEN prev_commit_date = date THEN 0\n",
    "                ELSE DATEDIFF(DAY, prev_commit_date, date) + 1\n",
    "            END AS time_diff\n",
    "        FROM commits_with_lag\n",
    "        ORDER BY date;   \n",
    "    \"\"\"\n",
    "    result = spark.sql(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "47cab4d5-496d-47d7-81e1-cf4f3b9c521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "expected = {\"count\": \"2532\",\n",
    "            \"stddev\": \"0.8288280854699502\",\n",
    "            \"max\": \"36\",\n",
    "            \"min\": \"0\",\n",
    "            \"mean\": \"0.045813586097946286\"}\n",
    "    \n",
    "result = assignment_4(\"GitHub\")\n",
    "\n",
    "time_diff_stats = result.describe(\"time_diff\").collect()\n",
    "\n",
    "for res in time_diff_stats:\n",
    "    assert res[\"time_diff\"] == expected[res[\"summary\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c65e64-b04a-4300-b25b-3f67041de202",
   "metadata": {},
   "source": [
    "### Assignment 5\n",
    "\n",
    "**Function Description**\n",
    "\n",
    "To gain some insight into Spark SQL and its aggregation functions, you are required to implement a function that returns a DataFrame containing two columns:\n",
    "\n",
    "- `day` (int): The day of the week based on the commit date, where Sunday is 1, Monday is 2, etc.\n",
    "- `commits_per_day`: The total number of commits made on that particular day of the week.\n",
    "\n",
    "### Expected DataFrame Example:\n",
    "\n",
    "| day | commits_per_day |\n",
    "|-----|-----------------|\n",
    "| 0   | 32              |\n",
    "| ... | ...             |\n",
    "\n",
    "### Parameters:\n",
    "\n",
    "- **commits**: A DataFrame containing the commit data. You can refer to `commit.json` and `data_raw.json` for the structure of the file, or run `println(commits.schema)` to inspect the schema.\n",
    "\n",
    "### Return:\n",
    "\n",
    "- A DataFrame containing:\n",
    "  - `day`: The day of the week as an integer.\n",
    "  - `commits_per_day`: The total number of commits made on that day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8b5bed28-d552-4a53-b0b3-23f83f0e2660",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assignment_5():\n",
    "    query = \"\"\"\n",
    "        SELECT \n",
    "            dayofweek(commit.committer.date) as day,\n",
    "            COUNT(*) as commits_per_day\n",
    "        FROM commits\n",
    "        GROUP BY day\n",
    "    \"\"\"\n",
    "    res = spark.sql(query)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "937e5f0f-77c8-4815-9a76-8d42cd687c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "result = assignment_5().collect()\n",
    "\n",
    "expected = {7:60, 4:373, 3:151, 1:70, 2:187, 5:9021, 6:138}\n",
    "\n",
    "for res in result:\n",
    "    assert res['commits_per_day'] == expected[res['day']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac75c4c0-603d-4a76-b6f9-c0e5c656b95a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
